{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#GLEU\n",
        "-Bu metrik, gramer hatası düzeltmesi için yapılan değişikliklerin kalitesini ölçer. Düzeltme yapılan kelime sayısı ve toplam kelime sayısı arasındaki orana ek olarak, düzeltilmiş cümlenin gramatik doğruluğunu da dikkate alır.\n",
        "\n",
        "-0 ile 1 arasında bir değerdir ve 1'e ne kadar yakınsa, gramer hatası düzeltme modelinin başarısı o kadar yüksektir."
      ],
      "metadata": {
        "id": "jLrryPq9vhcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.gleu_score import sentence_gleu\n",
        "from nltk.translate.gleu_score import corpus_gleu\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TlgH85OEqu4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"The quick brown fox jumps over the lazy dog\"\n",
        "hypothesis = \"The quick brown fox jumps over the lazier dog\"\n",
        "\n",
        "ref_tokenized = [word for word in reference.lower().split()]\n",
        "gen_tokenized = [word for word in hypothesis.lower().split()]\n",
        "\n"
      ],
      "metadata": {
        "id": "LJO6tgmwvPbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gleu_score = sentence_gleu([ref_tokenized], gen_tokenized)"
      ],
      "metadata": {
        "id": "kZ6-opIrvQz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(gleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0X_RjtZvSJK",
        "outputId": "e7e028dc-a950-45c6-ae42-ab14c2a15492"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#BLEU\n",
        "-BLEU skoru, n-gram'lerin örtüşen oranını hesaplar. İki cümle arasındaki benzerliği ölçmek için kullanılır.\n",
        "-Skorun maksimum değeri 1'dir ve ne kadar yüksek olursa, iki cümle arasındaki benzerliğin o kadar yüksek olduğu anlamına gelir.\n",
        "-Smoothing Function, BLUE skorunun hesaplanmasında kullanılan bir yöntemdir ve bazı durumlarda BLUE skorunun daha doğru bir şekilde hesaplanmasını sağlar.\n",
        "\n",
        "-Smoothing Function, özellikle nadir görülen kelimelerin veya n-gram'ların BLUE skoruna katkısını düzenler ve bazı durumlarda kesinlik değerini artırır. Bunun nedeni, BLUE skorunun hesaplanması sırasında, sadece model tarafından üretilen cümlede bulunan n-gram'lar değil, aynı zamanda gerçek cümlede bulunmayan n-gram'ların da değerlendirilmesidir. Bu nedenle, nadir görülen n-gram'lar varsa, BLUE skoru düşük olabilir.\n",
        "\n",
        "-BLEU metriğinde, n-gramlerin ağırlıkları önemlidir. weights değişkeni ile n-gramlere ağırlık verebiliriz. Burada, her bir n-gram'a eşit ağırlık vermek için 0.25 olarak ayarlandı.\n",
        "\n",
        "BLEU metriğinde ayrıca düzgünleştirme işlemi de yapılır. SmoothingFunction().method1 ile method1 yöntemi kullanılarak, çıktılar düzgünleştirilir.\n",
        "\n",
        "-BLEU metriği, n-gram benzerliği temel alarak makine çevirisi kalitesini ölçer. n-gram'ların sayısı, BLEU metriğinin hesaplanması için belirlenir. Tipik olarak, n değeri 1 ile 4 arasında değişen n-gram'lar kullanılır.\n",
        "\n",
        "Genellikle, BLEU metriği için 1-gram, 2-gram, 3-gram ve 4-gram'lar kullanılır. n değeri arttıkça, cümlelerin benzerliğini ölçmek daha zor hale gelir ve BLEU skoru düşer. Ancak, daha büyük n değerleri, daha yüksek düzeyde dilbilgisi doğruluğu sağlamak için gereklidir.\n",
        "\n",
        "n değeri, BLEU metriğinin performansını etkileyen bir faktördür ve bu değer, veri kümesi ve özellikle dil çiftine göre değişebilir. Tipik olarak, 1-gram ve 2-gram'ların daha fazla kullanıldığı görülür. Ancak, özellikle cümle uzunluğu uzadığında, 3-gram ve 4-gram'ların da kullanılması önerilir.\n",
        "\n",
        "-"
      ],
      "metadata": {
        "id": "wHsDTNSGv7Fm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sacrebleu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_snWqdrBLDcS",
        "outputId": "3cacecc3-205f-471d-ca5d-a5dc5c9de710"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (4.9.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (1.22.4)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.9/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"from sacrebleu.metrics import BLEU\n",
        "\n",
        "import sacrebleu\n",
        "\n",
        "reference = \"The quick brown fox jumps over the lazy dog\"\n",
        "hypothesis = \"The quick brown fox jumps over the lazier dog\"\n",
        "\n",
        "ref_tokenized = [word for word in reference.lower().split()]\n",
        "gen_tokenized = [word for word in hypothesis.lower().split()]\n",
        "\n",
        "\n",
        "gleu_score1 = sacrebleu.corpus_bleu(gen_tokenized,ref_tokenized)\n",
        "\n",
        "print(\"GLEU Score:\", gleu_score1)\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3zxtCwaLDls",
        "outputId": "78363827-cfb0-4b2e-e5ae-d0d273515b62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GLEU Score: BLEU = 0.00 0.0/0.0/0.0/0.0 (BP = 1.000 ratio = 1.000 hyp_len = 3 ref_len = 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu, SmoothingFunction\n"
      ],
      "metadata": {
        "id": "Q6nCr4WhHhPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"The quick brown fox jumps over the lazy dog\"\n",
        "hypothesis = \"The quick brown fox jumps over the lazy dog\"\n",
        "\n",
        "reference = [reference.split()]\n",
        "candidate = hypothesis.split()\n",
        "\n",
        "smoothing_function = SmoothingFunction().method2  # Çıktıların düzgünleştirilmesi için\n",
        "weights = (0.5, 0.5)  # 4-gram'lara eşit ağırlık vermek için\n",
        "\n",
        "bleu_score1 = sentence_bleu(reference, candidate, weights=weights, smoothing_function=smoothing_function)\n"
      ],
      "metadata": {
        "id": "6y9xlZEkIjpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bleu_score1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iql35pGrHi2m",
        "outputId": "3dcfa107-bb68-4074-9834-0a50e4358fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F1 skoru\n",
        "-Bir modelin doğruluğunu ölçmek için yaygın olarak kullanılan bir ölçüttür.\n",
        "-calculate_f1_score() işlevi, iki metni (reference ve hypothesis) parametre olarak alır ve F1 skorunu hesaplar.\n",
        "-İlk olarak, tp, fp ve fn kümeleri hesaplanır. Bu kümeler, doğru pozitifler, yanlış pozitifler ve yanlış negatifler olarak sırasıyla metinler arasındaki benzerlikleri temsil eder.\n",
        "-Daha sonra, precision, recall ve f1 değerleri hesaplanır.\n",
        "- F1 skoru, en yüksek doğruluğu sağlamak için hem doğruluğu hem de kapsamı birleştirir.\n",
        "\n"
      ],
      "metadata": {
        "id": "V_TW0DtVFgef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f1_score(reference, hypothesis):\n",
        "    # convert reference and hypothesis sentences to lists of words\n",
        "    ref_words = reference.strip().split()\n",
        "    hyp_words = hypothesis.strip().split()\n",
        "\n",
        "    # create a set of true positives, false positives, and false negatives\n",
        "    tp = set(ref_words) & set(hyp_words)\n",
        "    fp = set(hyp_words) - tp\n",
        "    fn = set(ref_words) - tp\n",
        "\n",
        "    # calculate precision, recall, and f1 score\n",
        "    precision = len(tp) / (len(tp) + len(fp))\n",
        "    recall = len(tp) / (len(tp) + len(fn))\n",
        "    f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "    return f1"
      ],
      "metadata": {
        "id": "adsz5I-dF2rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"The quick brown fox jumps over the lazy dog\"\n",
        "hypothesis = \"The quick brown fox jumps over the lazier dog\"\n"
      ],
      "metadata": {
        "id": "wmjXYXAaF8Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score1 = calculate_f1_score(reference, hypothesis)\n",
        "print(\"F1 score for hypothesis:\", f1_score1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXJu_CKlF_tz",
        "outputId": "03e08449-4c83-419c-8c33-ca076e7fb55f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 score for hypothesis: 0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Rouge Skoru\n",
        "-ROUGE metrikleri, referans metindeki önemli kelimelerin, aday metinde ne kadar iyi yansıtıldığını değerlendirir. ROUGE, \"n-gram\" adı verilen ve ardışık kelime gruplarını temsil eden özellikler kullanır. ROUGE-1, ROUGE-2 ve ROUGE-L olmak üzere üç ana ROUGE metriği vardır.\n",
        "\n",
        "-ROUGE-1, referans ve aday metinlerdeki tek kelime örtüşmelerinin yüzdesini ölçer. Bu metrik, kelime seviyesindeki benzerliği ölçer.\n",
        "-ROUGE-2, referans ve aday metinlerdeki iki kelime örtüşmelerinin yüzdesini ölçer. Bu metrik, kelime çiftleri seviyesindeki benzerliği ölçer.\n",
        "-ROUGE-L, Longest Common Subsequence (En Uzun Ortak Alt Dizi) yöntemini kullanarak, referans ve aday metinler arasındaki en uzun ortak alt dizinin uzunluğunu ölçer. Bu metrik, belge seviyesindeki benzerliği ölçer ve daha esnek bir eşleştirme yöntemi kullanarak, kelime seviyesindeki farklılıkları dikkate alır.\n",
        "-ROUGE, ayrıca precision (kesinlik) ve recall (duyarlılık) ölçüleri de kullanır. Precision, aday metindeki örtüşmelerin referans metindeki örtüşmelerden ne kadarını içerdiğini ölçerken, recall, referans metindeki örtüşmelerin aday metindeki örtüşmelerden ne kadarını içerdiğini ölçer. Bu iki ölçü, ROUGE metriklerinde \"p\" ve \"r\" olarak belirtilir ve \"f\" ölçüsü ile birleştirilerek ROUGE skoru hesaplanır."
      ],
      "metadata": {
        "id": "G6cPFgCfUZUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rouge\n"
      ],
      "metadata": {
        "id": "pWRKASa8O0XM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca1f96c-2f98-42b8-d709-91637fab301c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "reference = 'this movies were awesome'\n",
        "candidate = 'this movie was awesome '\n",
        "rouge = Rouge()\n"
      ],
      "metadata": {
        "id": "iQJuBy4IO0bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores = rouge.get_scores(candidate, reference)[0]\n",
        "rouge_1_f = scores['rouge-l']['f']\n",
        "print(rouge_1_f)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JN4eOzV0R1-j",
        "outputId": "cf6cda03-6144-445e-fe77-0f0e64829a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4999999950000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#METEOR"
      ],
      "metadata": {
        "id": "zrSDCN_cq3pB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7yRCwH8sHgb",
        "outputId": "72f16474-c82c-428f-f99e-98328c5e2314"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# İki cümleyi tanımlayın ve tokenize edin\n",
        "reference = \"The quick brown fox jumps over the lazy dog.\"\n",
        "hypothesis = \"A quick brown fox jumps over a lazy dog.\"\n",
        "ref_tokens = word_tokenize(reference)\n",
        "hyp_tokens = word_tokenize(hypothesis)\n",
        "\n",
        "# METEOR skorunu hesaplayın\n",
        "score = meteor_score([ref_tokens], hyp_tokens)\n",
        "\n",
        "print(\"METEOR skoru:\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV3UTiolq30Z",
        "outputId": "8e29bec1-1098-4b0c-a0be-8a555e8957fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "METEOR skoru: 0.7937500000000002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#F05"
      ],
      "metadata": {
        "id": "ZMxAYj_-O6fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "def calculate_f05_score(reference, hypothesis):\n",
        "    # convert reference and hypothesis sentences to lists of words\n",
        "    ref_words = reference.strip().split()\n",
        "    hyp_words = hypothesis.strip().split()\n",
        "\n",
        "    # create a set of true positives, false positives, and false negatives\n",
        "    tp = set(ref_words) & set(hyp_words)\n",
        "    fp = set(hyp_words) - tp\n",
        "    fn = set(ref_words) - tp\n",
        "\n",
        "    # calculate precision, recall, and f1 score\n",
        "    precision = len(tp) / (len(tp) + len(fp))\n",
        "    recall = len(tp) / (len(tp) + len(fn))\n",
        "# Calculate F-0.5\n",
        "    beta=0.5\n",
        "    f05_score = float((1 + (beta**2)) * precision * recall) / (((beta**2) * precision) + recall) if precision + recall else 0.0\n",
        "    return f05_score"
      ],
      "metadata": {
        "id": "MTJLpkiVNrla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference = \"The quick brown fox jumps over the lazy dog\"\n",
        "hypothesis = \"The quick brown fox jumps over the lazier dog\"\n"
      ],
      "metadata": {
        "id": "ayTQKnJQNrlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f05_score1 = calculate_f05_score(reference, hypothesis)\n",
        "print(\"F05 score for hypothesis:\", f05_score1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859c585e-0cba-45c3-c54c-cf5870b96f71",
        "id": "BzxlS1nONrlb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F05 score for hypothesis: 0.8888888888888888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9QDu3-YVNnA6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
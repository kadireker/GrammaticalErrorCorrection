{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install language-tool-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx9gp5bEZl9z",
        "outputId": "c18fda4d-2c13-474f-fa2e-ae5462baac56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting language-tool-python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language-tool-python) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language-tool-python) (3.4)\n",
            "Installing collected packages: language-tool-python\n",
            "Successfully installed language-tool-python-2.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from language_tool_python import LanguageTool\n",
        "from tqdm import tqdm\n",
        "import csv\n"
      ],
      "metadata": {
        "id": "LHK9sbs3V82X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aRAoI2gVUuP",
        "outputId": "cbf231f6-819f-4dd2-f40a-24ddfb30cb1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading LanguageTool 5.7: 100%|██████████| 225M/225M [00:07<00:00, 29.5MB/s]\n",
            "INFO:language_tool_python.download_lt:Unzipping /tmp/tmpqfbioo88.zip to /root/.cache/language_tool_python.\n",
            "INFO:language_tool_python.download_lt:Downloaded https://www.languagetool.org/download/LanguageTool-5.7.zip to /root/.cache/language_tool_python.\n"
          ]
        }
      ],
      "source": [
        "tool = LanguageTool('en-US')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"They is go to the cinema\"\n",
        "matches = tool.check(text)\n",
        "for match in matches:\n",
        "    corrected_text = text[:match.offset] + match.replacements[0] + text[match.offset + match.errorLength:]\n",
        "    print(corrected_text)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mVj6bLjTa_OX",
        "outputId": "bb6a9f96-4bc4-4139-ada2-c96b4014fca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "They are go to the cinema\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dosya_adı = \"exportLanguageTool_Correction.csv\"\n",
        "csv_satırları = []\n",
        "dd=0\n",
        "with open('/content/drive/MyDrive/Kadir_TEZ/sentence_2000_grammer_error.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=';')\n",
        "    dd=0\n",
        "    for col in tqdm(csv_reader):\n",
        "      print(str(dd)+\" \" +col[0])\n",
        "      print(str(dd)+ \"\" +col[1])\n",
        "      dd = dd+1"
      ],
      "metadata": {
        "id": "aONs-lvyefAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dosya_adı = \"exportLanguageTool_Correction_JFLEG.csv\"\n",
        "csv_satırları = []\n",
        "\n",
        "with open('/content/drive/MyDrive/Kadir_TEZ/jfleg_test.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
        "    for col in tqdm(csv_reader):\n",
        "      dd=dd+1\n",
        "      incorrect = col[1]\n",
        "      incorrect = incorrect.replace('\"', '')\n",
        "      correct = col[0]\n",
        "      correct = correct.replace('\"', '')\n",
        "      matches = tool.check(incorrect)\n",
        "      correctedLanguageTool=\"\"\n",
        "      try:\n",
        "        for match in matches:\n",
        "          corrected_text = incorrect[:match.offset] + match.replacements[0] + incorrect[match.offset + match.errorLength:]\n",
        "          correctedLanguageTool =corrected_text\n",
        "          break\n",
        "      except:\n",
        "        correctedLanguageTool=correct\n",
        "        print(\"hata var ama devamke\")\n",
        "\n",
        "      csv_satırları.append((correctedLanguageTool,correct))\n",
        "\n",
        "with open(dosya_adı, mode='w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file,delimiter='|')\n",
        "    for satir in csv_satırları:\n",
        "        writer.writerow(satir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9u2wrAL4Q7B",
        "outputId": "36e56836-67d1-419e-f30e-0addcf52fccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "84it [00:04, 22.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hata var ama devamke\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "748it [00:35, 21.07it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dosya_adı = \"exportLanguageTool_Correction.csv\"\n",
        "csv_satırları = []\n",
        "dd=0\n",
        "aa=0\n",
        "with open('/content/drive/MyDrive/Kadir_TEZ/sentence_2000_grammer_error.csv') as csv_file:\n",
        "    csv_reader = csv.reader(csv_file, delimiter=';')\n",
        "    for col in tqdm(csv_reader):\n",
        "      dd=dd+1\n",
        "      incorrect = col[0]\n",
        "      correct = col[1]\n",
        "      matches = tool.check(incorrect)\n",
        "      correctedLanguageTool=\"\"\n",
        "      try:\n",
        "        for match in matches:\n",
        "          corrected_text = incorrect[:match.offset] + match.replacements[0] + incorrect[match.offset + match.errorLength:]\n",
        "          correctedLanguageTool =corrected_text\n",
        "          break\n",
        "      except:\n",
        "        correctedLanguageTool=correct\n",
        "        print(\"hata var ama devamke\")\n",
        "\n",
        "      csv_satırları.append((correctedLanguageTool,correct))\n",
        "\n",
        "with open(dosya_adı, mode='w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file,delimiter='|')\n",
        "    for satir in csv_satırları:\n",
        "        writer.writerow(satir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYL-0Qtvb76l",
        "outputId": "ccad20e0-b92e-4ddb-84ff-7961b7b16339"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1779it [01:13, 33.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hata var ama devamke\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2000it [01:21, 24.55it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets\n"
      ],
      "metadata": {
        "id": "OadA_GKgxtmW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17befff5-f224-4aa2-96e6-951cf80d4b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, huggingface-hub, aiosignal, aiohttp, datasets\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.12.0 dill-0.3.6 frozenlist-1.3.3 huggingface-hub-0.14.1 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"jfleg\")"
      ],
      "metadata": {
        "id": "I6AD_TpWpqqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"test\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW1POw_Jyb9g",
        "outputId": "65d82c3a-90a9-4fac-9474-c1cbd5fd9a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['sentence', 'corrections'],\n",
              "    num_rows: 748\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = dataset[\"test\"]\n",
        "data = {\"sentence\": test_data[\"sentence\"], \"corrections\": test_data[\"corrections\"]}\n",
        "\n",
        "# DataFrame oluşturun\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "data = {\"corrections\": [ref[0] for ref in test_data[\"corrections\"]],\"sentence\": test_data[\"sentence\"]}\n",
        "\n",
        "# DataFrame oluşturun\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# DataFrame'i CSV dosyasına kaydedin\n",
        "df.to_excel(\"jfleg_test.xlsx\")\n"
      ],
      "metadata": {
        "id": "Ja0oRjr0x0Ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jfleg = load_dataset(\"jfleg\")\n",
        "\n",
        "# Test veri kümesini seçin\n",
        "test_data = jfleg[\"test\"]\n",
        "\n",
        "# Kaynak ve hedef cümleleri birleştirin\n",
        "data = {\"src\": test_data[\"sentence\"], \"ref\": test_data[\"corrections\"]}\n",
        "\n",
        "# DataFrame oluşturun\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# DataFrame'deki her bir satırı dolaşıp ekrana yazdırın\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"{row['ref'][0]}\")"
      ],
      "metadata": {
        "id": "LbHBHc-V474o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in df.iterrows():\n",
        "    print(f\"{row['ref'][0]}\")"
      ],
      "metadata": {
        "id": "NcZAcXVJ4caU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "046v2hFPIF_3",
        "outputId": "c5582d2f-2031-4ab1-c0d4-8e254c8a8e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.27.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai"
      ],
      "metadata": {
        "id": "4vataMCiIESf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dosya_adı = \"exportGPT35_Correction_JFLEG.csv\"\n",
        "csv_satırları = []\n",
        "openai.api_key = \"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\"\n",
        "for index, row in df.iterrows():\n",
        "    error_sentence = row['sentence']\n",
        "    model_engine = \"davinci\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=model_engine,\n",
        "        prompt=(f\"Please correct the following sentence:\\n\\n{error_sentence}\\n\"),\n",
        "        max_tokens=60,\n",
        "        n=1,\n",
        "        stop=None,\n",
        "        temperature=0.5,\n",
        "    )\n",
        "    output = response.choices[0].text.strip()\n",
        "    csv_satırları.append((output,error_sentence))\n",
        "\n",
        "with open(dosya_adı, mode='w', newline='') as csv_file:\n",
        "    writer = csv.writer(csv_file,delimiter='|')\n",
        "    for satir in csv_satırları:\n",
        "        writer.writerow(satir)"
      ],
      "metadata": {
        "id": "whmVrrrlIBtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv"
      ],
      "metadata": {
        "id": "2KZfGflNp7p-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excel_file = '/content/drive/MyDrive/Kadir_TEZ/bard_Correction360.xlsx'\n",
        "\n",
        "# İki sütunlu Excel dosyasını okuyun\n",
        "df = pd.read_excel(excel_file, usecols=[0,1])\n",
        "\n",
        "# CSV dosyasının adını ve konumunu belirleyin\n",
        "csv_file = 'bard_Correction.csv'\n",
        "\n",
        "# CSV dosyasına yazın\n",
        "df.to_csv(csv_file, index=False, sep='|')"
      ],
      "metadata": {
        "id": "czkAVZHS_olm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}